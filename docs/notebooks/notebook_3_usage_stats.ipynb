{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e65f2c7-7a69-48cc-ac62-64cc0d3fe260",
   "metadata": {},
   "source": [
    "# Notebook 3 - Get usage stats for a specific period\n",
    "\n",
    "You can get the notebook file [here](https://github.com/mila-iqia/SARC/blob/master/docs/notebooks/notebook_3_usage_stats.ipynb).\n",
    "\n",
    "This example provides a code to compute usage statistics for a specific period on a list of clusters.\n",
    "\n",
    "(based on examples/usage_stats.py)\n",
    "\n",
    "Letâ€™s first configure the `SARC_CONFIG` variable, as in notebook 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9a4f2-afc5-4660-a99f-9a11a5babeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "os.environ[\"SARC_CONFIG\"] = \"../../config/sarc-client.json\";\n",
    "print(os.environ[\"SARC_CONFIG\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91be591-7364-4f6a-ac40-cf71e22f9c01",
   "metadata": {},
   "source": [
    "Example will use pandas, which may print many warnings. Let's suppress them to get a more readable output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1001619-1f34-4170-9b09-89a9442a26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5d258-4870-4088-a46c-7601875d3482",
   "metadata": {},
   "source": [
    "Then the main code, which is inspired from script `examples/usage_stats.py` in repository root.\n",
    "\n",
    "To define the period, we will set a `start` time and `end` time using `datetime` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from sarc.config import MTL\n",
    "\n",
    "start = datetime.now(MTL) - timedelta(days=7)\n",
    "end = datetime.now(MTL)\n",
    "\n",
    "print(\"Start: \", start)\n",
    "print(\"End: \", end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6792ed",
   "metadata": {},
   "source": [
    "Then, we get the jobs and format it as a DataFrame.\n",
    "\n",
    "Note that this code may take time to run if clusters contain a lot of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d71c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sarc.config import config\n",
    "from sarc.jobs import get_jobs\n",
    "\n",
    "# Clusters for which we want to compute statistics. \n",
    "# For this example, we will use just 2 clusters.\n",
    "clusters = [\"mila\", \"narval\"]\n",
    "\n",
    "# Subset of slurm fields we need to compute the trends\n",
    "include_fields = {\n",
    "    \"cluster_name\",\n",
    "    \"user\",\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    \"elapsed_time\",\n",
    "    \"job_id\",\n",
    "    \"array_job_id\",\n",
    "    \"task_id\",\n",
    "    \"qos\",\n",
    "    \"partition\",\n",
    "}\n",
    "\n",
    "def get_jobs_dataframe(start, end) -> pd.DataFrame:\n",
    "\n",
    "    df = None\n",
    "    # Fetch all jobs from the clusters\n",
    "    for c, cluster in enumerate(clusters):\n",
    "        print(\"Getting job for cluster\", cluster, f\"({c + 1} / {len(clusters)})\")\n",
    "        dicts = []\n",
    "\n",
    "        # Precompute the total number of jobs to display a progress bar\n",
    "        # get_jobs is a generator so we don't get the total unless we pre-fetch all jobs\n",
    "        # beforehand.\n",
    "        total = config().mongo.database_instance.jobs.count_documents(\n",
    "            {\n",
    "                \"cluster_name\": cluster,\n",
    "                \"end_time\": {\"$gte\": start},\n",
    "                \"start_time\": {\"$lt\": end},\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        # Fetch all jobs from the cluster\n",
    "        jobs = get_jobs(cluster=cluster, start=start, end=end)\n",
    "        for job in tqdm(jobs,total=total,ncols=80):\n",
    "            \n",
    "            if job.elapsed_time <= 0:\n",
    "                continue\n",
    "\n",
    "            if job.end_time is None:\n",
    "                job.end_time = datetime.now(tz=MTL)\n",
    "\n",
    "            # For some reason start time is not reliable, often equal to submit time,\n",
    "            # so we infer it based on end_time and elapsed_time.\n",
    "            job.start_time = job.end_time - timedelta(seconds=job.elapsed_time)\n",
    "\n",
    "            # Clip the job to the time range we are interested in.\n",
    "            if job.start_time < start:\n",
    "                job.start_time = start\n",
    "            if job.end_time > end:\n",
    "                job.end_time = end\n",
    "            job.elapsed_time = (job.end_time - job.start_time).total_seconds()\n",
    "\n",
    "            # We only care about jobs that actually ran.\n",
    "            if job.elapsed_time <= 0:\n",
    "                continue\n",
    "\n",
    "            # Create a small dict with the fields we need\n",
    "            job_dict = job.dict(include=include_fields)\n",
    "            # Add the allocation fields directry to dicts instead of nested as in the original job dict.\n",
    "            job_dict.update(job.allocated.dict())\n",
    "\n",
    "            dicts.append(job_dict)\n",
    "\n",
    "        # Replace all NaNs by 0.\n",
    "        cluster_df = pd.DataFrame(dicts).fillna(0)\n",
    "        df = pd.concat([df, cluster_df])\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_jobs_dataframe(start=start, end=end)\n",
    "\n",
    "print(\"Number of jobs:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a46ed",
   "metadata": {},
   "source": [
    "Once we have our dataframe loaded, let's get some interresting values from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a8457-0de2-44e3-8fb8-ccc5b55101d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the billed and used resource time in seconds\n",
    "df[\"billed\"] = df[\"elapsed_time\"] * df[\"billing\"]\n",
    "df[\"used\"] = df[\"elapsed_time\"] * df[\"gres_gpu\"]\n",
    "\n",
    "df_mila = df[df[\"cluster_name\"] == \"mila\"]\n",
    "df_drac = df[df[\"cluster_name\"] != \"mila\"]\n",
    "\n",
    "print(\"Number of jobs:\".center(80,'='))\n",
    "print(\"Mila-cluster\", df_mila.shape[0])\n",
    "print(\"DRAC clusters\", df_drac.shape[0])\n",
    "\n",
    "print(\"GPU hours:\".center(80,'='))\n",
    "print(\"Mila-cluster\", df_mila[\"used\"].sum() / (3600))\n",
    "print(\"DRAC clusters\", df_drac[\"used\"].sum() / (3600))\n",
    "\n",
    "\n",
    "def compute_gpu_hours_per_duration(df):\n",
    "    categories = {\n",
    "        \"< 1hour\": (0, 3600),\n",
    "        \"1-24 hours\": (3600, 24 * 3600),\n",
    "        \"1-28 days\": (24 * 3600, 28 * 24 * 3600),\n",
    "        \">= 28 days\": (28 * 24 * 3600, None),\n",
    "    }\n",
    "    for key, (min_time, max_time) in categories.items():\n",
    "        condition = df[\"elapsed_time\"] >= min_time\n",
    "        if max_time is not None:\n",
    "            condition *= df[\"elapsed_time\"] < max_time\n",
    "        df[key] = condition.astype(bool) * df[\"used\"]\n",
    "\n",
    "    return df[list(categories.keys())].sum() / df[\"used\"].sum()\n",
    "\n",
    "\n",
    "print(\"GPU hours per job duration\".center(80,'='))\n",
    "print(\"Mila-cluster:\")\n",
    "print(compute_gpu_hours_per_duration(df_mila))\n",
    "print(\"DRAC clusters:\")\n",
    "print(compute_gpu_hours_per_duration(df_drac))\n",
    "\n",
    "\n",
    "def compute_jobs_per_gpu_hours(df):\n",
    "    categories = {\n",
    "        \"< 1 GPUhour\": (0, 3600),\n",
    "        \"1-24 GPUhours\": (3600, 24 * 3600),\n",
    "        \"1-28 GPUdays\": (24 * 3600, 28 * 24 * 3600),\n",
    "        \">= 28 GPUdays\": (28 * 24 * 3600, None),\n",
    "    }\n",
    "    for key, (min_time, max_time) in categories.items():\n",
    "        condition = df[\"used\"] >= min_time\n",
    "        if max_time is not None:\n",
    "            condition *= df[\"used\"] < max_time\n",
    "        df[key] = condition.astype(bool) * df[\"used\"]\n",
    "\n",
    "    return df[list(categories.keys())].sum() / df[\"used\"].sum()\n",
    "\n",
    "\n",
    "print(\"Binned GPU hours\".center(80,'='))\n",
    "print(\"Mila-cluster:\")\n",
    "print(compute_jobs_per_gpu_hours(df_mila))\n",
    "print(\"DRAC clusters:\")\n",
    "print(compute_jobs_per_gpu_hours(df_drac))\n",
    "\n",
    "\n",
    "def compute_gpu_hours_per_gpu_count(df):\n",
    "    categories = {\n",
    "        \"1 GPU\": (1, 2),\n",
    "        \"2-4 GPUs\": (2, 5),\n",
    "        \"5-8 GPUs\": (5, 9),\n",
    "        \"9-32 GPUs\": (9, 33),\n",
    "        \">= 33 PUdays\": (33, None),\n",
    "    }\n",
    "    for key, (min_time, max_time) in categories.items():\n",
    "        condition = df[\"gres_gpu\"] >= min_time\n",
    "        if max_time is not None:\n",
    "            condition *= df[\"gres_gpu\"] < max_time\n",
    "        df[key] = condition.astype(bool) * df[\"used\"]\n",
    "\n",
    "    return df[list(categories.keys())].sum() / df[\"used\"].sum()\n",
    "\n",
    "\n",
    "print(\"GPU hours per gpu job count\".center(80,'='))\n",
    "print(\"Mila-cluster:\")\n",
    "print(compute_gpu_hours_per_gpu_count(df_mila))\n",
    "print(\"DRAC clusters:\")\n",
    "print(compute_gpu_hours_per_gpu_count(df_drac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
