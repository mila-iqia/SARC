{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e65f2c7-7a69-48cc-ac62-64cc0d3fe260",
   "metadata": {},
   "source": [
    "# Notebook 3 - Get usage stats for a specific period\n",
    "\n",
    "This example provides a code to compute usage statistics for a specific period on a list of clusters.\n",
    "\n",
    "Letâ€™s first configure the `SARC_CONFIG` variable, as in notebook 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a9a4f2-afc5-4660-a99f-9a11a5babeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../config/sarc-client.json\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "os.environ[\"SARC_CONFIG\"] = \"../../config/sarc-client.json\";\n",
    "print(os.environ[\"SARC_CONFIG\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91be591-7364-4f6a-ac40-cf71e22f9c01",
   "metadata": {},
   "source": [
    "Example will use pandas, which may print many warnings. Let's suppress them to get a more readable output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1001619-1f34-4170-9b09-89a9442a26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5d258-4870-4088-a46c-7601875d3482",
   "metadata": {},
   "source": [
    "Then the main code, which is inspired from script `examples/usage_stats.py` in repository root.\n",
    "\n",
    "To define the period, we will set a `start` time and `end` time using `datetime` class.\n",
    "\n",
    "Note that this code may take time to run if clusters contain a lot of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993a8457-0de2-44e3-8fb8-ccc5b55101d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting job for cluster mila (1 / 2)\n",
      "[mila] Getting jobs 100000 / 929234\n",
      "[mila] Getting jobs 200000 / 929234\n",
      "[mila] Getting jobs 300000 / 929234\n",
      "[mila] Getting jobs 400000 / 929234\n",
      "[mila] Getting jobs 500000 / 929234\n",
      "[mila] Getting jobs 600000 / 929234\n",
      "[mila] Getting jobs 700000 / 929234\n",
      "[mila] Getting jobs 800000 / 929234\n",
      "[mila] Getting jobs 900000 / 929234\n",
      "[mila] Got jobs 929234 / 929234\n",
      "Getting job for cluster narval (2 / 2)\n",
      "[narval] Getting jobs 100000 / 652510\n",
      "[narval] Getting jobs 200000 / 652510\n",
      "[narval] Getting jobs 300000 / 652510\n",
      "[narval] Getting jobs 400000 / 652510\n",
      "[narval] Getting jobs 500000 / 652510\n",
      "[narval] Getting jobs 600000 / 652510\n",
      "[narval] Got jobs 652510 / 652510\n",
      "Number of jobs:\n",
      "Mila-cluster 743251\n",
      "DRAC clusters 562930\n",
      "GPU hours:\n",
      "Mila-cluster 3054313.8527777777\n",
      "DRAC clusters 1528260.2416666667\n",
      "GPU hours per job duration\n",
      "Mila-cluster:\n",
      "< 1hour       0.029210\n",
      "1-24 hours    0.461250\n",
      "1-28 days     0.443574\n",
      ">= 28 days    0.065966\n",
      "dtype: float64\n",
      "DRAC clusters:\n",
      "< 1hour       0.012006\n",
      "1-24 hours    0.496383\n",
      "1-28 days     0.491611\n",
      ">= 28 days    0.000000\n",
      "dtype: float64\n",
      "Binned GPU hours\n",
      "Mila-cluster:\n",
      "< 1 GPUhour      0.027818\n",
      "1-24 GPUhours    0.431027\n",
      "1-28 GPUdays     0.455548\n",
      ">= 28 GPUdays    0.085607\n",
      "dtype: float64\n",
      "DRAC clusters:\n",
      "< 1 GPUhour      0.011141\n",
      "1-24 GPUhours    0.424799\n",
      "1-28 GPUdays     0.531043\n",
      ">= 28 GPUdays    0.033017\n",
      "dtype: float64\n",
      "GPU hours per gpu job count\n",
      "Mila-cluster:\n",
      "1 GPU           8.008571e-01\n",
      "2-4 GPUs        1.687451e-01\n",
      "5-8 GPUs        2.665919e-02\n",
      "9-32 GPUs       3.738617e-03\n",
      ">= 33 PUdays    3.637842e-09\n",
      "dtype: float64\n",
      "DRAC clusters:\n",
      "1 GPU           0.766141\n",
      "2-4 GPUs        0.224896\n",
      "5-8 GPUs        0.008931\n",
      "9-32 GPUs       0.000033\n",
      ">= 33 PUdays    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sarc.config import MTL, config\n",
    "from sarc.jobs import get_jobs\n",
    "\n",
    "# Clusters for which we want to compute statistics. \n",
    "# For this example, we will use just 2 clusters.\n",
    "clusters = [\"mila\", \"narval\"]\n",
    "\n",
    "# Subset of slurm fields we need to compute the trends\n",
    "include_fields = {\n",
    "    \"cluster_name\",\n",
    "    \"user\",\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    \"elapsed_time\",\n",
    "    \"job_id\",\n",
    "    \"array_job_id\",\n",
    "    \"task_id\",\n",
    "    \"qos\",\n",
    "    \"partition\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_jobs_dataframe(start, end) -> pd.DataFrame:\n",
    "\n",
    "    df = None\n",
    "    # Fetch all jobs from the clusters\n",
    "    for c, cluster in enumerate(clusters):\n",
    "        print(\"Getting job for cluster\", cluster, f\"({c + 1} / {len(clusters)})\")\n",
    "        dicts = []\n",
    "\n",
    "        # Precompute the total number of jobs to display a progress bar\n",
    "        # get_jobs is a generator so we don't get the total unless we pre-fetch all jobs\n",
    "        # beforehand.\n",
    "        total = config().mongo.database_instance.jobs.count_documents(\n",
    "            {\n",
    "                \"cluster_name\": cluster,\n",
    "                \"end_time\": {\"$gte\": start},\n",
    "                \"start_time\": {\"$lt\": end},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for i, job in enumerate(get_jobs(cluster=cluster, start=start, end=end)):\n",
    "            if (i + 1) % 100000 == 0:\n",
    "                print(f\"[{cluster}] Getting jobs {i + 1} / {total}\")\n",
    "\n",
    "            if job.elapsed_time <= 0:\n",
    "                continue\n",
    "\n",
    "            if job.end_time is None:\n",
    "                job.end_time = datetime.now(tz=MTL)\n",
    "\n",
    "            # For some reason start time is not reliable, often equal to submit time,\n",
    "            # so we infer it based on end_time and elapsed_time.\n",
    "            job.start_time = job.end_time - timedelta(seconds=job.elapsed_time)\n",
    "\n",
    "            # Clip the job to the time range we are interested in.\n",
    "            if job.start_time < start:\n",
    "                job.start_time = start\n",
    "            if job.end_time > end:\n",
    "                job.end_time = end\n",
    "            job.elapsed_time = (job.end_time - job.start_time).total_seconds()\n",
    "\n",
    "            # We only care about jobs that actually ran.\n",
    "            if job.elapsed_time <= 0:\n",
    "                continue\n",
    "\n",
    "            # Create a small dict with the fields we need\n",
    "            job_dict = job.dict(include=include_fields)\n",
    "            # Add the allocation fields directry to dicts instead of nested as in the original job dict.\n",
    "            job_dict.update(job.allocated.dict())\n",
    "\n",
    "            dicts.append(job_dict)\n",
    "\n",
    "        print(f\"[{cluster}] Got jobs {total} / {total}\")\n",
    "\n",
    "        # Replace all NaNs by 0.\n",
    "        cluster_df = pd.DataFrame(dicts).fillna(0)\n",
    "        df = pd.concat([df, cluster_df])\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "start = datetime(year=2022, month=1, day=1, tzinfo=MTL)\n",
    "end = datetime(year=2023, month=1, day=1, tzinfo=MTL)\n",
    "df = get_jobs_dataframe(start=start, end=end)\n",
    "\n",
    "# Compute the billed and used resource time in seconds\n",
    "df[\"billed\"] = df[\"elapsed_time\"] * df[\"billing\"]\n",
    "df[\"used\"] = df[\"elapsed_time\"] * df[\"gres_gpu\"]\n",
    "\n",
    "df_mila = df[df[\"cluster_name\"] == \"mila\"]\n",
    "df_drac = df[df[\"cluster_name\"] != \"mila\"]\n",
    "\n",
    "print(\"Number of jobs:\")\n",
    "print(\"Mila-cluster\", df_mila.shape[0])\n",
    "print(\"DRAC clusters\", df_drac.shape[0])\n",
    "\n",
    "print(\"GPU hours:\")\n",
    "print(\"Mila-cluster\", df_mila[\"used\"].sum() / (3600))\n",
    "print(\"DRAC clusters\", df_drac[\"used\"].sum() / (3600))\n",
    "\n",
    "\n",
    "def compute_gpu_hours_per_duration(df):\n",
    "    categories = {\n",
    "        \"< 1hour\": (0, 3600),\n",
    "        \"1-24 hours\": (3600, 24 * 3600),\n",
    "        \"1-28 days\": (24 * 3600, 28 * 24 * 3600),\n",
    "        \">= 28 days\": (28 * 24 * 3600, None),\n",
    "    }\n",
    "    for key, (min_time, max_time) in categories.items():\n",
    "        condition = df[\"elapsed_time\"] >= min_time\n",
    "        if max_time is not None:\n",
    "            condition *= df[\"elapsed_time\"] < max_time\n",
    "        df[key] = condition.astype(bool) * df[\"used\"]\n",
    "\n",
    "    return df[list(categories.keys())].sum() / df[\"used\"].sum()\n",
    "\n",
    "\n",
    "print(\"GPU hours per job duration\")\n",
    "print(\"Mila-cluster:\")\n",
    "print(compute_gpu_hours_per_duration(df_mila))\n",
    "print(\"DRAC clusters:\")\n",
    "print(compute_gpu_hours_per_duration(df_drac))\n",
    "\n",
    "\n",
    "def compute_jobs_per_gpu_hours(df):\n",
    "    categories = {\n",
    "        \"< 1 GPUhour\": (0, 3600),\n",
    "        \"1-24 GPUhours\": (3600, 24 * 3600),\n",
    "        \"1-28 GPUdays\": (24 * 3600, 28 * 24 * 3600),\n",
    "        \">= 28 GPUdays\": (28 * 24 * 3600, None),\n",
    "    }\n",
    "    for key, (min_time, max_time) in categories.items():\n",
    "        condition = df[\"used\"] >= min_time\n",
    "        if max_time is not None:\n",
    "            condition *= df[\"used\"] < max_time\n",
    "        df[key] = condition.astype(bool) * df[\"used\"]\n",
    "\n",
    "    return df[list(categories.keys())].sum() / df[\"used\"].sum()\n",
    "\n",
    "\n",
    "print(\"Binned GPU hours\")\n",
    "print(\"Mila-cluster:\")\n",
    "print(compute_jobs_per_gpu_hours(df_mila))\n",
    "print(\"DRAC clusters:\")\n",
    "print(compute_jobs_per_gpu_hours(df_drac))\n",
    "\n",
    "\n",
    "def compute_gpu_hours_per_gpu_count(df):\n",
    "    categories = {\n",
    "        \"1 GPU\": (1, 2),\n",
    "        \"2-4 GPUs\": (2, 5),\n",
    "        \"5-8 GPUs\": (5, 9),\n",
    "        \"9-32 GPUs\": (9, 33),\n",
    "        \">= 33 PUdays\": (33, None),\n",
    "    }\n",
    "    for key, (min_time, max_time) in categories.items():\n",
    "        condition = df[\"gres_gpu\"] >= min_time\n",
    "        if max_time is not None:\n",
    "            condition *= df[\"gres_gpu\"] < max_time\n",
    "        df[key] = condition.astype(bool) * df[\"used\"]\n",
    "\n",
    "    return df[list(categories.keys())].sum() / df[\"used\"].sum()\n",
    "\n",
    "\n",
    "print(\"GPU hours per gpu job count\")\n",
    "print(\"Mila-cluster:\")\n",
    "print(compute_gpu_hours_per_gpu_count(df_mila))\n",
    "print(\"DRAC clusters:\")\n",
    "print(compute_gpu_hours_per_gpu_count(df_drac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
